{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556db79f",
   "metadata": {
    "id": "556db79f"
   },
   "source": [
    "# Compute Attention map for ViT tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9f008",
   "metadata": {
    "id": "d5c9f008"
   },
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686c3d1",
   "metadata": {
    "id": "e686c3d1"
   },
   "source": [
    "Attention formulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a8485",
   "metadata": {
    "id": "330a8485"
   },
   "source": [
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        & Attention(Q,K,V) = softmax \\left(\\frac{QK^{T}}{\\sqrt d_{k}}\\right)V\n",
    "            % \\min_{G}\\max_{D}\\mathbf{E}_{x\\sim p_{data}(x)}[log(D(x))] + \\mathbf{E}_{z\\sim p_{z}(z)}[log(1-D(G(z)))]\n",
    "            % KL(P \\| Q) \\coloneqq \\int_{-\\infty}^{\\infty} log \\frac{P(dx)}{Q(dx)}P(dx)\n",
    "    \\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdaaf05",
   "metadata": {
    "id": "1fdaaf05"
   },
   "source": [
    "Self-attention takes an input vector, and transform them with $W_{q}$, $W_{k}$ and $W_{v}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4d929d",
   "metadata": {
    "id": "0d4d929d"
   },
   "source": [
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        & SelfAttention(x) = softmax \\left(\\frac{xW_{q}W_{k}^{T}x^{T}}{\\sqrt d_{k}}\\right)xW_{v}\n",
    "            % \\min_{G}\\max_{D}\\mathbf{E}_{x\\sim p_{data}(x)}[log(D(x))] + \\mathbf{E}_{z\\sim p_{z}(z)}[log(1-D(G(z)))]\n",
    "            % KL(P \\| Q) \\coloneqq \\int_{-\\infty}^{\\infty} log \\frac{P(dx)}{Q(dx)}P(dx)\n",
    "    \\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357247d3",
   "metadata": {
    "id": "357247d3"
   },
   "source": [
    "## Understand ViT tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531c3f2",
   "metadata": {
    "id": "0531c3f2"
   },
   "source": [
    "![alt text](https://raw.githubusercontent.com/schwartz-cnl/Computational-Neuroscience-Class/refs/heads/main/Transformers%20and%20Self-Attention/assets/vit_architecture.jpg \"ViT architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4059f224",
   "metadata": {
    "id": "4059f224"
   },
   "source": [
    "As opposed to tokenizing words as in LLMs, Vision transformers tokenize the image in sequential patches but insert a CLS token for learning classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac439b3e",
   "metadata": {
    "id": "ac439b3e"
   },
   "source": [
    "## Understand attention score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc742e07",
   "metadata": {
    "id": "cc742e07"
   },
   "source": [
    "Attention score is defined per token in the attention formulation, the normalized attention score is simply:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21552d2e",
   "metadata": {
    "id": "21552d2e"
   },
   "source": [
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        softmax \\left(\\frac{QK^{T}}{\\sqrt d_{k}}\\right)\n",
    "    \\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc2524a",
   "metadata": {
    "id": "ccc2524a"
   },
   "source": [
    "### How to visualize attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92005b",
   "metadata": {
    "id": "1b92005b"
   },
   "source": [
    "1. Mean attention distance: distance weighted attention score.\n",
    "2. Attention Rollout: recursively scaling weighted attention score from output to input.\n",
    "3. Attention heatmaps: visualize normalized attention score per token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd56ff1",
   "metadata": {
    "id": "cbd56ff1"
   },
   "source": [
    "In this lab, we will visualize the attention map with respect to the CLS token to get a better idea of what the model is attending to when it comes to classification.\n",
    "\n",
    "For further challenges and understanding, we will examine intra-token attentions based on some metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb4feb",
   "metadata": {
    "id": "74cb4feb"
   },
   "source": [
    "## Lab: visualize attention heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79e3ead",
   "metadata": {
    "id": "b79e3ead"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174ba2d",
   "metadata": {
    "id": "7174ba2d"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v2 as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf7bdf",
   "metadata": {
    "id": "99bf7bdf"
   },
   "source": [
    "### Load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe9dc3",
   "metadata": {
    "id": "49fe9dc3"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "GITHUB_RELEASE = \"https://github.com/sayakpaul/probing-vits/releases/download/v1.0.0/probing_vits.zip\"\n",
    "FNAME = \"probing_vits.zip\"\n",
    "MODELS_ZIP = {\n",
    "    \"vit_dino_base16\": \"Probing_ViTs/vit_dino_base16.zip\",\n",
    "    \"vit_b16_patch16_224\": \"Probing_ViTs/vit_b16_patch16_224.zip\",\n",
    "    \"vit_b16_patch16_224-i1k_pretrained\": \"Probing_ViTs/vit_b16_patch16_224-i1k_pretrained.zip\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d15cc",
   "metadata": {
    "id": "b71d15cc"
   },
   "outputs": [],
   "source": [
    "zip_path = tf.keras.utils.get_file(\n",
    "    fname=FNAME,\n",
    "    origin=GITHUB_RELEASE,\n",
    ")\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "\n",
    "os.rename(\"Probing ViTs\", \"Probing_ViTs\")\n",
    "\n",
    "with zipfile.ZipFile(\"Probing_ViTs/vit_b16_patch16_224-i1k_pretrained.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"Probing_ViTs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf724c29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf724c29",
    "outputId": "91af2ef5-d3b7-4758-ff5e-9782d652c648"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./Probing_ViTs/vit_b16_patch16_224-i1k_pretrained')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0127a5",
   "metadata": {
    "id": "6f0127a5"
   },
   "source": [
    "### Preprocess the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b7b87",
   "metadata": {
    "id": "449b7b87"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa451c8",
   "metadata": {
    "id": "1aa451c8"
   },
   "outputs": [],
   "source": [
    "input_resolution = 224\n",
    "\n",
    "norm_layer = tf.keras.layers.Normalization(\n",
    "    mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "    variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_image(image, size=input_resolution):\n",
    "    image = np.array(image)\n",
    "    image_resized = tf.expand_dims(image, 0)\n",
    "    image_resized = tf.image.resize(\n",
    "        image_resized, (size, size), method=\"bicubic\"\n",
    "    )\n",
    "    if image_resized.shape[-1] == 1:\n",
    "        norm_img = image_resized\n",
    "    else:\n",
    "        norm_img = norm_layer(image_resized).numpy()\n",
    "    return norm_img, image_resized\n",
    "\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(io.BytesIO(response.content))\n",
    "    if len(np.array(image).shape) == 2:\n",
    "        image_np = np.expand_dims(np.array(image), axis=-1)\n",
    "    else:\n",
    "        image_np = np.array(image)\n",
    "    preprocessed_image, resized_image = preprocess_image(image_np)\n",
    "    return image, preprocessed_image, resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cce35",
   "metadata": {
    "id": "907cce35"
   },
   "outputs": [],
   "source": [
    "# Load image from public url\n",
    "image, preprocessed_image, resized_image = load_image_from_url(\n",
    "    \"https://dl.fbaipublicfiles.com/dino/img.png\"\n",
    ")\n",
    "normalized_resize = (resized_image-np.min(resized_image))/(np.max(resized_image)-np.min(resized_image))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383339da",
   "metadata": {
    "id": "383339da"
   },
   "outputs": [],
   "source": [
    "### TO DO: upload your own image to a public url and visualize its attention score. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc14e6e",
   "metadata": {
    "id": "cbc14e6e"
   },
   "source": [
    "### Extract Attention Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf9b67",
   "metadata": {
    "id": "1aaf9b67"
   },
   "outputs": [],
   "source": [
    "# List of the names of the transformer blocks\n",
    "blocknames = ['transformer_block_0_att',\n",
    "                  'transformer_block_1_att',\n",
    "                  'transformer_block_2_att',\n",
    "                  'transformer_block_3_att',\n",
    "                  'transformer_block_4_att',\n",
    "                  'transformer_block_5_att',\n",
    "                  'transformer_block_6_att',\n",
    "                  'transformer_block_7_att',\n",
    "                  'transformer_block_8_att',\n",
    "                  'transformer_block_9_att',\n",
    "                  'transformer_block_10_att',\n",
    "                  'transformer_block_11_att',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a305f4fd",
   "metadata": {
    "id": "a305f4fd"
   },
   "outputs": [],
   "source": [
    "# Extract and sort attention score by layer\n",
    "attention_score_dict = model(preprocessed_image, training=False)[1]\n",
    "attention_score_list=[]\n",
    "for k in blocknames:\n",
    "    attention_score_list.append(attention_score_dict[k])\n",
    "\n",
    "nh = 12\n",
    "token_size = int(224/16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d97a6a4",
   "metadata": {
    "id": "2d97a6a4"
   },
   "source": [
    "### Visualize heatmap for last layer per head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1f454",
   "metadata": {
    "id": "2de1f454"
   },
   "outputs": [],
   "source": [
    "# Pick attention score from specific layers\n",
    "attention = attention_score_list[-1].numpy()\n",
    "\n",
    "# Process attention score into heatmap\n",
    "attention = attention[0, :, 0, 1:]\n",
    "attention = attention.reshape(nh, token_size, token_size)\n",
    "attention = attention.transpose((1, 2, 0))\n",
    "attention = tf.image.resize(\n",
    "                attention,\n",
    "                size=(\n",
    "                    224,\n",
    "                    224,\n",
    "                ),\n",
    "            ).numpy()\n",
    "attention = attention.transpose((2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f671dca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "id": "3f671dca",
    "outputId": "f01b5080-a0ce-404c-fa56-ebe043767242"
   },
   "outputs": [],
   "source": [
    "# Plot image and heat map per head\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(13, 13))\n",
    "img_count = 0\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        if img_count < len(attention):\n",
    "            axes[i, j].imshow(normalized_resize[0])\n",
    "            axes[i, j].imshow(attention[img_count], alpha=0.5)\n",
    "            axes[i, j].title.set_text(f\"Attention head: {img_count}\")\n",
    "            axes[i, j].axis(\"off\")\n",
    "            img_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba16ad8",
   "metadata": {
    "id": "bba16ad8"
   },
   "outputs": [],
   "source": [
    "### TO_DO: Visualize attention heatmap for other layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0dfea",
   "metadata": {
    "id": "cfd0dfea"
   },
   "source": [
    "### Optional: examine attention heatmap for any arbitrary token pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a86bf7",
   "metadata": {
    "id": "08a86bf7"
   },
   "outputs": [],
   "source": [
    "### TO DO: compare all tokens pair-wise and look for semantically meaningful attentions with ground truth masks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49f4e2",
   "metadata": {
    "id": "1f49f4e2"
   },
   "source": [
    "### Load image and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360b15f",
   "metadata": {
    "id": "6360b15f"
   },
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "num_tokens = int(input_resolution/patch_size)**2+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653cd023",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "653cd023",
    "outputId": "b96c75a3-e15e-4a7b-c568-aab092a8d0c3"
   },
   "outputs": [],
   "source": [
    "image, preprocessed_image, resized_image = load_image_from_url(\n",
    "    # \"https://raw.githubusercontent.com/schwartz-cnl/Computational-Neuroscience-Class/refs/heads/main/Transformers%20and%20Self-Attention/assets/beer.jpg\"\n",
    "    # \"https://raw.githubusercontent.com/schwartz-cnl/Computational-Neuroscience-Class/refs/heads/main/Transformers%20and%20Self-Attention/assets/chicks.jpg\"\n",
    "    \"https://raw.githubusercontent.com/schwartz-cnl/Computational-Neuroscience-Class/refs/heads/main/Transformers%20and%20Self-Attention/assets/duck.jpg\"\n",
    ")\n",
    "normalized_resize = (resized_image-np.min(resized_image))/(np.max(resized_image)-np.min(resized_image))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cc9a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "393cc9a2",
    "outputId": "3c36376a-e867-486b-ec1e-527bab2cc411"
   },
   "outputs": [],
   "source": [
    "image_dist, _, distractor = load_image_from_url(\n",
    "    # \"https://raw.githubusercontent.com/schwartz-cnl/Computational-Neuroscience-Class/refs/heads/main/Transformers%20and%20Self-Attention/assets/beer_dist.jpg\"\n",
    "    # \"https://raw.githubusercontent.com/schwartz-cnl/Computational-Neuroscience-Class/refs/heads/main/Transformers%20and%20Self-Attention/assets/chicks_dist.jpg\"\n",
    "    \"https://raw.githubusercontent.com/schwartz-cnl/Computational-Neuroscience-Class/refs/heads/main/Transformers%20and%20Self-Attention/assets/duck_dist.jpg\"\n",
    ")\n",
    "normalized_distractor = (distractor-np.min(distractor))/(np.max(distractor)-np.min(distractor))\n",
    "image_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1697a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "15e1697a",
    "outputId": "7a0025ab-f520-4554-d010-461c3dc41910"
   },
   "outputs": [],
   "source": [
    "image_targ, _, target = load_image_from_url(\n",
    "    # \"https://raw.githubusercontent.com/schwartz-cnl/Computational-Neuroscience-Class/refs/heads/main/Transformers%20and%20Self-Attention/assets/beer_targ.jpg\"\n",
    "    # \"https://raw.githubusercontent.com/schwartz-cnl/Computational-Neuroscience-Class/refs/heads/main/Transformers%20and%20Self-Attention/assets/chicks_targ.jpg\"\n",
    "    \"https://raw.githubusercontent.com/schwartz-cnl/Computational-Neuroscience-Class/refs/heads/main/Transformers%20and%20Self-Attention/assets/duck_targ.jpg\"\n",
    ")\n",
    "normalized_target = (target-np.min(target))/(np.max(target)-np.min(target))\n",
    "image_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e77be8",
   "metadata": {
    "id": "c6e77be8"
   },
   "outputs": [],
   "source": [
    "# Set color map\n",
    "from matplotlib.colors import colorConverter\n",
    "import matplotlib as mpl\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# Generate the transparent colors\n",
    "color1 = colorConverter.to_rgba('white',alpha=0.0)\n",
    "color2 = colorConverter.to_rgba('red',alpha=0.8)\n",
    "color3 = colorConverter.to_rgba('cyan',alpha=0.8)\n",
    "\n",
    "# Make the colormaps\n",
    "cmap1 = mpl.colors.LinearSegmentedColormap.from_list('my_cmap',[color1,color2],256)\n",
    "cmap2 = mpl.colors.LinearSegmentedColormap.from_list('my_cmap2',[color1,color3],256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z9QMacRgzd8Q",
   "metadata": {
    "id": "z9QMacRgzd8Q"
   },
   "source": [
    "credits:\n",
    "1. adapted from https://keras.io/examples/vision/probing_vits/#method-iii-attention-heatmaps\n",
    "2. methods adpated from https://arxiv.org/abs/2405.14880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XA-wmYXtzx1H",
   "metadata": {
    "id": "XA-wmYXtzx1H"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
